<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    
    <!-- Website verification -->
    <meta name="google-site-verification" content="IgpZiPgMZSPdIAtVIcZ7mIxFioE2C5d_GTnrjJTOPmA">
<!-- Avoid warning on Google Chrome
        Error with Permissions-Policy header: Origin trial controlled feature not enabled: 'interest-cohort'.
        see https://stackoverflow.com/a/75119417
    -->
    <meta http-equiv="Permissions-Policy" content="interest-cohort=()">

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Scaling Laws in Deep Learning | Aseem Pratap Subedi</title>
    <meta name="author" content="Aseem Pratap Subedi">
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/axiom5_triangle.ico">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://aseem.github.io/blog/2025/scaling-laws-in-deep-learning/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Aseem </span>Pratap Subedi</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Scaling Laws in Deep Learning</h1>
    <p class="post-meta">March 8, 2025• Aseem Subedi</p>
    <p class="post-tags">
      <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>
        ·  
        <a href="/blog/tag/ai">
          <i class="fas fa-hashtag fa-sm"></i> AI,</a>  
          <a href="/blog/tag/scaling">
          <i class="fas fa-hashtag fa-sm"></i> scaling</a>  
          <a href="/blog/tag/laws">
          <i class="fas fa-hashtag fa-sm"></i> laws,</a>  
          <a href="/blog/tag/llms">
          <i class="fas fa-hashtag fa-sm"></i> LLMs,</a>  
          <a href="/blog/tag/fingers">
          <i class="fas fa-hashtag fa-sm"></i> fingers</a>  
          

    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <div style="margin: 20px 0; text-align: center;">
  <img src="/assets/img/sun-and-star.jpg" alt="Image description" style="width: 100%; border-radius: 10px;">
</div>

<p>“Scaling law” sounds technical, but it’s pretty straightforward: make ‘em bigger, feed ‘em more, they get better (unlike us who actually start functioning slower after a burrito with extra chicken). Well, mostly.</p>

<h3 id="physics-dropping-knowledge-again">Physics Dropping Knowledge (Again)</h3>

<p>Believe it or not, the notion of scaling isn’t exclusive to deep learning. Physicists have long observed that as you increase the size or energy of a system, its behavior changes in predictable—and sometimes dramatic—ways. Imagine heating a single water droplet versus a whole pot of water: the boiling process isn’t just a scaled-up version of what happens in that tiny droplet. That’s the vibe we’re talking about in AI. This predictability in change is the essence of scaling laws in physics, and it provided the inspiration for AI researchers to ask: what happens if we just keep making our neural networks bigger and better?</p>

<div style="margin: 20px 0; text-align: center;">
  <img src="/assets/img/scaling-laws-of-small-l-3333176480.jpg" alt="Image description" style="width: 50%; border-radius: 10px;">
</div>

<h3 id="early-clues-translation-was-onto-something">Early Clues: Translation Was Onto Something</h3>

<p>Turns out, scale was the secret sauce.  Back in the day, researchers were wrestling with machine translation. And they noticed, the bigger the model, the more data it saw, the better it translated. Duh! But it was a big deal at the time.</p>

<p>This paper from 2014, “<a href="https://www.google.com/search?q=https://proceedings.neurips.cc/paper_files/paper/2014/file/a14ac55a4f27472c5d89029b8231495a-Paper.pdf" rel="external nofollow noopener" target="_blank">Sequence to sequence learning with neural networks</a>”, kinda hinted at this.  Sutskever, Vinyals, and Le showed that bigger “sequence-to-sequence” models, trained on loads of text, just translated better.  No “scaling laws” label yet, but the scale effect was obvious.</p>

<h3 id="the-scaling-laws-paper--formulas-and-stuff">The Scaling Laws Paper:  Formulas and Stuff</h3>

<p>Then, 2020 rolls around, and OpenAI drops “<a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://www.google.com/url?sa=E%26source=gmail%26q=https://www.google.com/url?sa=E%26source=gmail%26q=http://arxiv.org/pdf/2001.08361" rel="external nofollow noopener" target="_blank">Scaling Laws for Neural Language Models</a>”.  Kaplan and co. actually put numbers on this scaling thing.  Formulas, even.</p>

<p>Turns out, performance improves predictably when you scale:</p>

<ul>
  <li>
<strong>Model Size (N):</strong>  Bigger brain. More connections.  You get it.</li>
  <li>
<strong>Dataset Size (D):</strong> More data gives the model a richer tapestry of patterns to learn from.</li>
  <li>
<strong>Compute (C):</strong> The raw horsepower behind the training process.</li>
</ul>

<p>They even gave us some rough formulas, showing loss goes down as you crank up N, D, and C.  Numbers like <em>αN</em> ≈ 0.076, <em>αD</em> ≈ 0.095, <em>αC</em> ≈ 0.050 floating around.  In other words, the more you invest in these areas, the better your model gets—but with diminishing returns as you push the limits.</p>

<div style="margin: 20px 0; text-align: center;">
  <img src="/assets/img/7lhht8n.png" alt="Image description" style="width: 80%; border-radius: 10px;">
</div>

<h3 id="smarter-scaling-is-a-thing">Smarter Scaling is a Thing</h3>

<p>So, just keep making models huge?  Not <em>exactly</em>.  DeepMind chimed in 2022 with the “Chinchilla scaling” idea.  Paper’s called “<a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://www.google.com/url?sa=E%26source=gmail%26q=https://arxiv.org/pdf/2203.15556" rel="external nofollow noopener" target="_blank">Training compute-optimal large language models</a>” if you’re digging deeper.  Basically, they argued maybe we were making models <em>too</em> big, not using data efficiently.  They suggested for the same compute, you might be better off with a slightly smaller model but <em>way</em> more data.  Smarter scaling, not just brute force. It’s a bit like cooking stew: you can’t just pile on ingredients without considering how they blend together. Efficient scaling is about striking the right balance rather than simply reaching for the largest pot with every condiment thrown in.</p>

<p>Scale still rules, but it’s nuanced.  It’s about balancing size, data, and compute.  People are still figuring out the details:</p>

<ul>
  <li>
<strong>Inference Compute:</strong>  Giving models more “think time” even <em>after</em> training can boost performance.</li>
  <li>
<strong>Better Architectures:</strong>  Designing models that are just inherently better learners, getting more bang for your buck from data and compute.</li>
  <li>
<strong>Beyond Simple Formulas:</strong>  Power laws are cool, but maybe there’s more to the story. Sadly, the compute to do such analysis isnt available to every independent researcher :’(</li>
</ul>

<h3 id="are-we-gonna-max-out">Are We Gonna Max Out?</h3>

<p>Though scaling’s been good to us, it poses the question if we are then heading for an <strong>“information ceiling?”</strong>  Basically, is there only so much info out there to train on?  Once AI’s seen it all, does scaling even matter anymore?</p>

<p>Debate’s still raging.  But some things to consider:</p>

<ul>
  <li>
<strong>Data Quality is King:</strong>  More data isn’t always better data.  Crappy data in, crappy AI out.</li>
  <li>
<strong>Emergence vs. Understanding:</strong>  These models are doing impressive stuff.  But are they <em>actually</em> understanding, or just mimicking patterns?  Scaling might not bridge that gap. (Emergence is a way cooler and mystical enigma if you are interested!)</li>
  <li>
<strong>Compute Costs (Planet Too):</strong>  Training these monsters is expensive and energy-hungry.  Endless scaling wouldn’t be so sustainable.</li>
</ul>

<h3 id="why-vision-models-so-bad">Why Vision models so bad?:</h3>
<p>We’ve all seen generative models tweaking and tripping, which (unless they are purely diffusion-models) funnily enough, also use transformer architectures as backbone.</p>

<ul>
  <li>
<strong>Them fingers:</strong> Motion of legs is bad, and fingers are rendered even worse. I honestly haven’t found a reasonable answer, but there are theories.</li>
  <li>
<strong>Dial Clocks:</strong>  All of them show 10 past 10, since all models were trained with watch faces available in the internet, and all of them clocks do tell 10:10:30. See for yourself.</li>
  <li>
<strong>Wine in glass:</strong>  All of them are half full, aren’t they? Same. Pictures from the internet</li>
  <li>
<strong>Maths and “Strawberry”:</strong>  One of the reasons openAI’s O1 was code-named strawberry was for the infamous problem of initial LLMs not understanding this question (of how many r’s are there in a strawberry). Believe it or not, LLMs sucked at maths for the same reason (kinda): Tokenization.</li>
</ul>

<p>There is a ceiling from the data available to us so far, since we have used most of them up! New research will move towards new ways to make AI truly intelligent, not just… big.  It’s gonna be interesting, we are finally going to move past the overhyped stuff.</p>

<div style="margin: 20px 0; text-align: center;">
  <img src="/assets/img/dental-scaling.jpg" alt="Remember to get your teeth scaled" style="width: 80%; border-radius: 10px;">
</div>

<p><strong>References:</strong></p>

<ul>
  <li>Sutskever et al. (2014): “<a href="https://www.google.com/search?q=https://proceedings.neurips.cc/paper_files/paper/2014/file/a14ac55a4f27472c5d89029b8231495a-Paper.pdf" rel="external nofollow noopener" target="_blank">Sequence to sequence learning with neural networks</a>” -  Early hints of scale’s power in translation.</li>
  <li>Kaplan et al. (2020): “<a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://www.google.com/url?sa=E%26source=gmail%26q=https://www.google.com/url?sa=E%26source=gmail%26q=http://arxiv.org/pdf/2001.08361" rel="external nofollow noopener" target="_blank">Scaling Laws for Neural Language Models</a>” -  The scaling laws bible.</li>
  <li>Hoffmann et al. (2022): “<a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://www.google.com/url?sa=E%26source=gmail%26q=https://arxiv.org/pdf/2203.15556" rel="external nofollow noopener" target="_blank">Training compute-optimal large language models</a>” -  Scaling, but make it efficient.</li>
</ul>

    </div>
  </article>


</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Aseem Pratap Subedi. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.
Last updated: March 08, 2025.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-E5SJEGB1B0"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-E5SJEGB1B0');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
